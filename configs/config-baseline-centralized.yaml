# FILE: configs/config-baseline-centralized.yaml
# @package _global_
# CENTRALIZED CONFIGURATION
# Single source of truth for all settings
# Use experiment configs for overrides
# If overrides are needed, define experiment in defaults below
defaults:
  - _self_
  - dataset: dialogue_data
  - model: kobart
  - preprocessing: swap_unbiased_speaker
  - postprocessing: swap_unbiased_speaker
  - pytorch: default
  - inference: generation
  - experiment: null

# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
experiment_name: "config-baseline-centralized"
run_name: null  # Auto-generated
seed: 42

# =============================================================================
# EXPERIMENT TRACKING
# =============================================================================
wandb:
  project: "dialogue-summarization(wb2x)"
  entity: "boot_camp_13_2nd_group_2nd"
  username: "wchoi189@gmail.com"
  tags: ["kobart", "centralized", "main config"]
  notes: "This is Main config: config-baseline-centralized.yaml"
  offline: false
  log_model: false
  watch: false

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
dataset:
  # Paths
  data_path: "/home/wb2x/workspace/dialogue-summarizer/data"
  files:
    train: "train.csv"
    dev: "dev.csv" 
    test: "test.csv"
    submission_template: "sample_submission.csv"

  # Column mappings
  columns:
    id: "fname"
    input: "dialogue"
    target: "summary"
    topic: "topic"

  # Batch settings
  batch_size: 16
  eval_batch_size: 32
  num_workers: 8
  pin_memory: true
  shuffle_train: true
  shuffle_val: false
  drop_last: false

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
model:
  # Model selection
  name: "kobart"
  architecture: "bart"
  model_name_or_path: "digit82/kobart-summarization"

  tokenizer:
    # Tokenizer (same as model)
    name_or_path: "digit82/kobart-summarization"
    use_fast: true

  # Model compilation
  compile:
    enabled: true
    mode: "default"
    dynamic: true

# =============================================================================
# PREPROCESSING CONFIGURATION
# =============================================================================
# Removed the redundant block here. This is now handled by the `defaults` list

# =============================================================================
# POSTPROCESSING CONFIGURATION
# =============================================================================
# Removed the redundant block here. This is now handled by the `defaults` list

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  accelerator: "auto"
  devices: 1
  precision: "16-mixed"
  max_steps: -1
  max_epochs: 12
  accumulate_grad_batches: 4
  overfit_batches: 0.0
  limit_train_batches: 1.0
  limit_val_batches: 1.0

  # Optimizer
  optimizer:
    name: "adamw"
    lr: 1e-5
    weight_decay: 0.05
    betas: [0.9, 0.999]
    eps: 1e-8

  # Scheduler
  lr_scheduler:
    name: "cosine"
    warmup_ratio: 0.1

  # Validation
  val_check_interval: 1.0
  check_val_every_n_epoch: 1

  # Model saving
  monitor: "val/rouge_f"
  mode: "max"
  save_top_k: 1

  # Early stopping
  early_stopping:
    enabled: true
    monitor: "val/rouge_f"
    mode: "max"
    patience: 3
    min_delta: 0.01
    verbose: true

  # Gradient clipping
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"

  # For debug overrides
  fast_dev_run: false

# =============================================================================
# INFERENCE CONFIGURATION
# =============================================================================
inference:
  checkpoint_path: null

# =============================================================================
# GENERATION CONFIGURATION
# =============================================================================
generation:
  max_length: 40
  min_length: 5
  num_beams: 5
  repetition_penalty: 1.7
  length_penalty: 0.5
  early_stopping: true
  do_sample: false
  temperature: 1.0
  top_k: 50
  top_p: 1.0

# =============================================================================
# OUTPUT DIRECTORIES
# =============================================================================
output_dir: "outputs"
paths:
  log_dir: "${output_dir}/logs"
  model_dir: "${output_dir}/models"
  prediction_dir: "${output_dir}/predictions"

# =============================================================================
# LOGGING
# =============================================================================
logging:
  level: "INFO"
  use_rich: true
  file: "${paths.log_dir}/${experiment_name}.log"
  log_every_n_steps: 50

# =============================================================================
# PYTORCH SETTINGS
# =============================================================================
pytorch:
  float32_matmul_precision: "medium"
  cudnn_benchmark: true
  empty_cache_steps: 50