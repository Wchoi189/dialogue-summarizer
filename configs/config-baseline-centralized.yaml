# @package _global_
# CENTRALIZED CONFIGURATION - configs/config.yaml
# Single source of truth for all settings

defaults:
  - _self_  # This config has highest priority
  - experiment: baseline  # Load experiment-specific overrides

# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
experiment_name: "dialogue-summarization"
run_name: null  # Auto-generated
seed: 42

# =============================================================================
# DATA CONFIGURATION
# =============================================================================
dataset:
  # Paths
  data_path: "/home/wb2x/workspace/dialogue-summarizer/data"
  files:
    train: "train.csv"
    dev: "dev.csv" 
    test: "test.csv"
    submission_template: "sample_submission.csv"

  preprocessing:
    max_input_length: 512
    max_target_length: 60
    truncation: true
    padding: false # Dynamic padding is handled by the collate function
    normalize_whitespace: true
    remove_extra_newlines: true 
    special_tokens:
      - "#Person1#"
      - "#Person2#"
      - "#Person3#"
      - "#Person4#"
      - "#Person5#"
      - "#Person6#"
      - "#Person7#"    
      - "#PhoneNumber#"
      - "#Address#"
      - "#PassportNumber#"


  # Column mappings
  columns:
    id: "fname"
    input: "dialogue"
    target: "summary"
    topic: "topic"
  
  # Batch settings
  batch_size: 16
  eval_batch_size: 32
  num_workers: 8
  pin_memory: true
  shuffle_train: true
  shuffle_val: false
  drop_last: false

# =============================================================================
# MODEL CONFIGURATION  
# =============================================================================
model:
  # Model selection
  name: "kobart"
  architecture: "bart"
  model_name_or_path: "digit82/kobart-summarization"
  
  tokenizer:
    # Tokenizer (same as model)
    name_or_path: "digit82/kobart-summarization"
    use_fast: true

  
  # Model compilation
  compile:
    enabled: true
    mode: "default"
    dynamic: true  # Crucial for handling variable sequence lengths in NLP tasks

  
# =============================================================================
# TEXT PROCESSING CONFIGURATION
# =============================================================================

## Moved to DATA CONFIGURATION

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================
training:
  accelerator: "auto"
  devices: 1
  precision: "16-mixed"
  max_steps: -1  
  max_epochs: 8
  accumulate_grad_batches: 4
  overfit_batches: 0.0
  limit_train_batches: 1.0
  limit_val_batches: 1.0

  # Optimizer
  optimizer:
    name: "adamw"
    lr: 3e-6  # Conservative for pretrained model
    weight_decay: 0.05
    betas: [0.9, 0.999]
    eps: 1e-8
  
  # Scheduler
  lr_scheduler:
    name: "cosine"
    warmup_ratio: 0.1
  
  # Validation
  val_check_interval: 1.0
  check_val_every_n_epoch: 1
  
  # Model saving
  monitor: "val/rouge_f"
  mode: "max"
  save_top_k: 1
  
  # Early stopping
  early_stopping:
    enabled: true
    monitor: "val/rouge_f"
    mode: "max"
    patience: 5
    min_delta: 0.01
    verbose: true
  
  # Gradient clipping
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  
  # For debug overrides
  fast_dev_run: false
# =============================================================================
# GENERATION CONFIGURATION (SINGLE SOURCE OF TRUTH)
# =============================================================================
generation:
  # Length control
  max_length: 50  
  min_length: 8   # Minimum meaningful summary
  
  # Beam search
  num_beams: 5
  early_stopping: true
  
  # Quality control
  no_repeat_ngram_size: 3
  repetition_penalty: 1.5
  length_penalty: 0.5  # Favor shorter outputs
  
  # Sampling (disabled)
  do_sample: false
  temperature: 1.0
  top_k: 50
  top_p: 1.0

# =============================================================================
# EXPERIMENT TRACKING
# =============================================================================
wandb:
  project: "dialogue-summarization(wb2x)"
  entity: "boot_camp_13_2nd_group_2nd"
  username: "wchoi189@gmail.com"
  tags: ["baseline", "kobart", "centralized"]
  notes: "Using config-baseline-centralized.yaml"
  offline: false
  log_model: false
  watch: false

# =============================================================================
# OUTPUT DIRECTORIES
# =============================================================================
output_dir: "outputs"
paths:
  log_dir: "${output_dir}/logs"
  model_dir: "${output_dir}/models" 
  prediction_dir: "${output_dir}/predictions"

# =============================================================================
# LOGGING
# =============================================================================
logging:
  level: "INFO"
  use_rich: true
  file: "${paths.log_dir}/${experiment_name}.log"
  log_every_n_steps: 50

# =============================================================================
# PYTORCH SETTINGS
# =============================================================================
pytorch:
  float32_matmul_precision: "medium"
  cudnn_benchmark: true
  empty_cache_steps: 50