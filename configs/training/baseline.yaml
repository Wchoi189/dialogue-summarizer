# @package training
# Performance-oriented training configuration for a SINGLE GPU

# Training strategy
strategy: "auto"             # ‚óÄÔ∏è REVERTED: 'auto' is best for a single GPU
accelerator: "auto"
devices: 1                   # ‚óÄÔ∏è REVERTED: Target your single available GPU
precision: "16-mixed"

# Epochs and steps
max_epochs: 10          # ‚Üê REDUCED from 20 to prevent overfitting
max_steps: -1
min_epochs: 1

# Batch sizes
# Effective batch size is 8 * 4 = 32. Smaller batches for better gradients.
batch_size: 8           # ‚Üê REDUCED from 16 for more frequent updates
eval_batch_size: 16
accumulate_grad_batches: 4  # ‚Üê INCREASED from 2 to maintain effective batch size

# Optimizer
optimizer:
  name: "adamw"
  lr: 1e-5              # ‚Üê INCREASED from 1e-5 (was too low for base model)
  weight_decay: 0.01    # ‚Üê REDUCED from 0.05
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Learning rate scheduler
lr_scheduler:
  name: "cosine"
  warmup_ratio: 0.1
  warmup_steps: null
  max_lr: ${training.optimizer.lr}
  min_lr: 1.0e-7

# Validation and evaluation
val_check_interval: 1.0
check_val_every_n_epoch: 1
evaluation_strategy: "epoch"
predict_with_generate: true

# In configs/training/baseline.yaml
generation:
  max_length: 120        # ‚Üê REDUCED from 150 (more reasonable for Korean)
  min_length: 15         # ‚Üê REDUCED from 20 (but still force minimum length)
  num_beams: 4           # ‚Üê CHANGED from 1 to 4
  no_repeat_ngram_size: 3
  early_stopping: true   # ‚Üê CHANGED from false
  do_sample: false
  repetition_penalty: 1.4  # ‚Üê INCREASED from 1.3 to further reduce repetition
  length_penalty: 1.2    # ‚Üê REDUCED from 1.5 for better length control

# Model saving
save_strategy: "epoch"
save_total_limit: 5
save_top_k: 3
monitor: "val/rouge_f"
mode: "max"
save_on_train_epoch_end: false

# Early stopping
early_stopping:
  enabled: true
  monitor: "val/rouge_f"
  patience: 2          # ‚Üê REDUCED from 3 - stop early if not improving
  min_delta: 0.005     # ‚Üê INCREASED from 0.001 - require more significant improvement
  mode: "max"
  verbose: true

# Gradient settings
gradient_clip_val: 1.0
gradient_clip_algorithm: "norm"

# Logging
log_every_n_steps: 100 # LOG SPAM
# logging_strategy: "epoch"

# Memory optimization
dataloader_num_workers: 16  # üöÄ CHANGED: Increased based on your 32-core CPU to speed up data loading
dataloader_pin_memory: true
persistent_workers: true

# Mixed precision
amp_backend: "native"
amp_level: "O1"

# Debugging
fast_dev_run: false
overfit_batches: 0.0
limit_train_batches: 1.0
limit_val_batches: 1.0
limit_test_batches: 1.0

# Profiler
profiler: null  # Options: null, "simple", "advanced", "pytorch"

# Reproducibility settings
deterministic: false  # Set to true for full reproducibility (may slow training)
benchmark: true       # Set to false if input sizes vary significantly

# Resume training
resume_from_checkpoint: null
auto_resume: false