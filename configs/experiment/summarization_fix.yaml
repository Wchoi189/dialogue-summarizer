# configs/experiment/summarization_fix.yaml
# @package _global_
# Fix model to generate summaries instead of continuations

# CRITICAL: Force summarization behavior
training:
  max_epochs: 8
  optimizer:
    lr: 1e-4  # ðŸ”¥ Higher learning rate for faster convergence
    weight_decay: 0.01
  
  early_stopping:
    patience: 3
    min_delta: 0.05  # Require substantial ROUGE improvement

# Force proper input/output lengths based on EDA
dataset:
  preprocessing:
    max_input_length: 512
    max_target_length: 80   # ðŸ”¥ REDUCED: Based on EDA (16 words â‰ˆ 80 tokens)
  
  batch_size: 16
  eval_batch_size: 32

# Force summarization with strict length control
generation:
  max_length: 80    # ðŸ”¥ CRITICAL: Match target length (16 words â‰ˆ 80 tokens)
  min_length: 10    # ðŸ”¥ Force minimum summarization 
  num_beams: 5
  
  # CRITICAL: Aggressive summarization forcing
  length_penalty: 2.0       # ðŸ”¥ STRONG penalty for long outputs
  repetition_penalty: 1.8   # ðŸ”¥ Prevent repetitive generation
  no_repeat_ngram_size: 3
  early_stopping: true
  
  # Force completion
  do_sample: false
  temperature: 1.0

# Enhanced postprocessing for summarization
postprocessing:
  remove_tokens:
    - "<usr>"
    - "<s>"
    - "</s>"
    - "<pad>"
    - "<unk>"
  
  text_cleaning:
    strip_whitespace: true
    normalize_whitespace: true
    remove_empty_lines: true
    remove_repetitive_phrases: true  # ðŸ”¥ Key for summarization
  
  korean_specific:
    remove_special_markers: false  # Keep #Person# for context
    normalize_punctuation: true
  
  advanced:
    min_length: 8                    # ðŸ”¥ Based on EDA minimum
    max_length: 100                  # ðŸ”¥ Hard cutoff
    fix_incomplete_sentences: true
    force_korean_endings: true       # ðŸ”¥ Ensure proper Korean endings

# WandB tracking
wandb:
  tags: ["summarization_fix", "length_control", "korean_focused"]
  notes: "Fix model to generate summaries not continuations - based on EDA insights"