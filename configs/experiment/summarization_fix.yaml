# configs/experiment/summarization_fix.yaml
# @package _global_
# Fix model to generate summaries instead of continuations
training:
  max_epochs: 8
  optimizer:
    lr: 1e-4  # ðŸ”¥ Higher learning rate for faster convergence
    weight_decay: 0.01
  early_stopping:
    patience: 3
    min_delta: 0.05  # Require substantial ROUGE improvement

dataset:
  preprocessing:
    max_target_length: 80 # ðŸ”¥ REDUCED: Based on EDA (16 words â‰ˆ 80 tokens)
  batch_size: 16
  eval_batch_size: 32

generation:
  max_length: 80 # ðŸ”¥ CRITICAL: Match target length (16 words â‰ˆ 80 tokens)
  min_length: 10
  length_penalty: 2.0 # ðŸ”¥ STRONG penalty for long outputs
  repetition_penalty: 1.8 # ðŸ”¥ Prevent repetitive generation
  no_repeat_ngram_size: 3
  early_stopping: true
  do_sample: false

# Enhanced postprocessing for summarization
postprocessing:
  advanced:
    min_length: 8
    max_length: 100
    fix_incomplete_sentences: true
    force_korean_endings: true

# WandB tracking
wandb:
  tags: ["summarization_fix", "length_control", "korean_focused"]
  notes: "Fix model to generate summaries not continuations - based on EDA insights"