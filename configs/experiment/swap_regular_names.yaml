# FILE: configs/experiment/swap_regular_names.yaml
# @package _global_
# defaults:
#   - /config-baseline-centralized # Inherit from the base config
 
  # - /preprocessing/standard # Load the standard preprocessing settings
  # - /postprocessing/default # Load the default post-processing settings
# Override the preprocessing section with your token swapping strategy

# WandB tracking
wandb:
  tags: ["using experiment", "potential bias", "kobart"]
  notes: "This is experiment config: token swapping experiment - using natural names"
# =============================================================================
# EXPERIMENT METADATA
# =============================================================================
experiment_name: "swap_unbiased_speaker"


# and add your token_swapping settings to it.
preprocessing:
  strategy: "std"
  max_input_length: 512
  max_target_length: 60
  truncation: true
  padding: false
  normalize_whitespace: true
  remove_extra_newlines: true
  special_tokens:
    - "#Person1#"
    - "#Person2#"
    - "#Person3#"
    - "#Person4#"
    - "#Person5#"
    - "#Person6#"
    - "#Person7#"
    - "#PhoneNumber#"
    - "#Address#"
    - "#PassportNumber#"
  token_swapping:
    enable: true
    token_map:
      "#Person1#": "민준"
      "#Person2#": "서연"
      "#Person3#": "지훈"
      "#Person4#": "수진"
      "#Person5#": "현우"
      "#Person6#": "은영"
      "#Person7#": "동현"


postprocessing:
  skip_special_tokens: false
  remove_tokens:
    - "<usr>"
    - "<s>"
    - "</s>"
    - "<pad>"
    - "<unk>"
  text_cleaning:
    strip_whitespace: true
    normalize_whitespace: true
    remove_empty_lines: true
  korean_specific:
    remove_special_markers: false
    normalize_punctuation: true
  token_swapping:
    enable: true
    token_map:
      "#Person1#": "민준"
      "#Person2#": "서연"
      "#Person3#": "지훈"
      "#Person4#": "수진"
      "#Person5#": "현우"
      "#Person6#": "은영"
      "#Person7#": "동현"
# ✅ ADD THIS BLOCK if you want this experiment
# to use different training settings.
# training:
#   max_epochs: 20
#   optimizer:
#     lr: 2e-5