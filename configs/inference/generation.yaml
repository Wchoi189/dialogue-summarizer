# FILE: configs/inference/generation.yaml
# @package generation
# Top-level generation configuration

# Length control
max_length: 50
min_length: 5
max_new_tokens: null # Use max_length by default

# Beam search
num_beams: 5
num_beam_groups: 2
diversity_penalty: 0.0

# Sampling (disabled by default for deterministic output)
do_sample: false
temperature: 1.0
top_k: 50
top_p: 1.0

# Repetition control
repetition_penalty: 1.6
no_repeat_ngram_size: 3
encoder_no_repeat_ngram_size: 0

# Early stopping and penalty
early_stopping: true
length_penalty: 0.6

# Special tokens - will be set by the model/tokenizer
pad_token_id: null
bos_token_id: null
eos_token_id: null

# Output control
output_attentions: false
output_hidden_states: false
output_scores: false
return_dict_in_generate: false