# @package model
# KoBART model configuration

name: "kobart"
architecture: "bart"

# Model path/name from HuggingFace
model_name_or_path: "digit82/kobart-summarization"  # Revert to working model


# Model parameters
# REMOVED: All parameter modifications that break the model
parameters:
  # # BART specific
  # encoder_layers: 12
  # decoder_layers: 12
  # encoder_attention_heads: 16
  # decoder_attention_heads: 16
  # encoder_ffn_dim: 3072
  # decoder_ffn_dim: 3072
  # d_model: 768
  
  # Vocabulary
  vocab_size: null  # Will be set from tokenizer
  
  # Special tokens
  bos_token: "<s>"
  eos_token: "</s>"
  pad_token: "<pad>"
  unk_token: "<unk>"

# Tokenizer settings
tokenizer:
  name_or_path: "digit82/kobart-summarization"  # Use same model for tokenizer
  use_fast: true
  add_special_tokens: true
  
  # Length constraints
  max_source_length: 512
  max_target_length: 100
  

# Model initialization
init_weights: true
tie_word_embeddings: true

# Model specific settings for training
training_mode:
  gradient_checkpointing: true
  use_cache: false  # Disable during training
  
# Model specific settings for inference
inference_mode:
  use_cache: true
  output_attentions: false
  output_hidden_states: false

# Model compilation (PyTorch 2.0+)
compile:
  enabled: true  # âœ… CHANGE from false
  mode: "default"
  dynamic: false