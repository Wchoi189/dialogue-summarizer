# @package model
# Optimized configuration for the gogamza/kobart-base-v2 model

name: "kobart"
architecture: "bart"

# ✅ FIX: Change to a summarization-specific model
model_name_or_path: "digit82/kobart-summarization"

parameters:
  vocab_size: null

tokenizer:
  # ✅ FIX: Ensure the tokenizer matches the new model
  name_or_path: "digit82/kobart-summarization"
  use_fast: true
  add_special_tokens: true
  
  # Length constraints
  max_source_length: 1024
  max_target_length: 120

  # Repetition control
  repetition_penalty: 1.4
  no_repeat_ngram_size: 3
  
  # Early stopping and length penalty
  early_stopping: true
  length_penalty: 0.7 # ← ADDED to encourage shorter summaries

# Model initialization
init_weights: true
tie_word_embeddings: true

# Model specific settings for training
training_mode:
  # Reduces memory usage at the cost of a small amount of speed. Recommended.
  gradient_checkpointing: true
  use_cache: false  # Disable during training for efficiency

# Model specific settings for inference
inference_mode:
  use_cache: true
  output_attentions: false
  output_hidden_states: false

# Model compilation (PyTorch 2.0+)
compile:
  enabled: true
  mode: "default"
  # Crucial for handling variable sequence lengths in NLP tasks.
  dynamic: true